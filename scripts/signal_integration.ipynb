{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae572ff-627e-4d75-8488-20314736e675",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from snorkel.labeling import labeling_function\n",
    "from snorkel.labeling.model import LabelModel\n",
    "from snorkel.labeling import PandasLFApplier\n",
    "from snorkel.labeling import LFAnalysis\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def dicttrie(arr):\n",
    "    trie = {}\n",
    "    for p in tqdm(arr):\n",
    "        key = p.strip()\n",
    "        trie[key] = True\n",
    "    return trie\n",
    "\n",
    "def build_and_save_trie(data, file_path):\n",
    "    mytrie = dicttrie(data)\n",
    "\n",
    "    with open(file_path, \"wb\") as f:\n",
    "        pickle.dump(mytrie, f)\n",
    "    print(\"Save Done\")\n",
    "\n",
    "def plot_probabilities_histogram(Y):\n",
    "    plt.hist(Y, bins=10)\n",
    "    plt.xlabel(\"Probability of HIGH\")\n",
    "    plt.ylabel(\"Number of data points\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f0f583-506e-467a-a7e9-986a00fe4b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "\n",
    "methods = [\"clip\", \"dfn\", \"tmars\", \"hype\"]\n",
    "for method in methods:\n",
    "    # PATH for each score file\n",
    "    path = f\"/<FILE_PATH>/{method}.jsonl.gz\"\n",
    "    \n",
    "    table = pyarrow.json.read_json(path, pyarrow.json.ReadOptions(block_size=10 << 20))\n",
    "    \n",
    "    columns = [\"key\", f'{method}']\n",
    "    df_ = table.select(columns).to_pandas()\n",
    "\n",
    "    if len(df) == 0:\n",
    "        df = df_\n",
    "    else:\n",
    "        prev_len = len(df)\n",
    "        df = df.merge(df_[[\"key\", f\"{method}\"]], on=\"key\")\n",
    "        assert len(df) == prev_len\n",
    "\n",
    "# dataframe df contains quality scores for each data sample\n",
    "scaler = MinMaxScaler()\n",
    "df[methods] = scaler.fit_transform(df[methods])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3efac583-416f-43d2-93ba-5877d0e6fd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimal thresholds\n",
    "b_c, b_d, b_t, b_h = 0.2, 0.1, 0.1, 0.1\n",
    "bandwidth = 0.25\n",
    "\n",
    "clip_th1 = np.percentile(df[\"clip\"], 100 - b_c * (1-bandwidth) * 100)\n",
    "clip_th0 = np.percentile(df[\"clip\"], 100 - b_c * (1+bandwidth) * 100)\n",
    "\n",
    "dfn_th1 = np.percentile(df[\"dfn\"], 100 - b_d * (1-bandwidth) * 100)\n",
    "dfn_th0 = np.percentile(df[\"dfn\"], 100 - b_d * (1+bandwidth) * 100)\n",
    "\n",
    "tmars_th1 = np.percentile(df[\"tmars\"], 100 - b_t * (1-bandwidth) * 100)\n",
    "tmars_th0 = np.percentile(df[\"tmars\"], 100 - b_t * (1+bandwidth) * 100)\n",
    "\n",
    "hype_th1 = np.percentile(df[\"hype\"], 100 - b_h * (1-ratio) * 100)\n",
    "hype_th0 = np.percentile(df[\"hype\"], 100 - b_h * (1+ratio) * 100)\n",
    "hype_th = np.percentile(df[\"hype\"], 100 - b_h * (1) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b18af6-1452-44ff-8867-6ac99185cd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "HIGH = 1\n",
    "LOW = 0\n",
    "ABSTAIN = -1\n",
    "\n",
    "@labeling_function()\n",
    "def clip_filter(x):\n",
    "    if x[\"clip\"] >= clip_th1:\n",
    "        return HIGH\n",
    "    elif x[\"clip\"] >= clip_th0:\n",
    "        return ABSTAIN\n",
    "    else:\n",
    "        return LOW        \n",
    "        \n",
    "@labeling_function()\n",
    "def dfn_filter(x):\n",
    "    if x[\"dfn\"] >= dfn_th1:\n",
    "        return HIGH\n",
    "    elif x[\"dfn\"] >= dfn_th0:\n",
    "        return ABSTAIN\n",
    "    else:\n",
    "        return LOW\n",
    "\n",
    "@labeling_function()\n",
    "def tmars_filter(x):\n",
    "    if x[\"tmars\"] >= tmars_th1:\n",
    "        return HIGH\n",
    "    elif x[\"tmars\"] >= tmars_th0:\n",
    "        return ABSTAIN\n",
    "    else:\n",
    "        return LOW        \n",
    "        \n",
    "@labeling_function()\n",
    "def hype_filter(x):\n",
    "    if x[\"hype\"] >= hype_th1:\n",
    "        return HIGH\n",
    "    elif x[\"hype\"] >= hype_th0:\n",
    "        return ABSTAIN\n",
    "    else:\n",
    "        return LOW    \n",
    "        \n",
    "lfs = [clip_filter, dfn_filter, tmars_filter, hype_filter]\n",
    "\n",
    "applier = PandasLFApplier(lfs)\n",
    "L_train = applier.apply(df)\n",
    "\n",
    "# LFAnalysis(L=L_train, lfs=lfs).lf_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eccd001-7a0a-4a70-8fd8-49fcc186cd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_model = LabelModel(cardinality=HIGH+1, verbose=True, device=\"cuda\")\n",
    "label_model.fit(L_train, n_epochs=500, seed=123, lr=0.01, log_freq=50)\n",
    "\n",
    "probs_train = label_model.predict_proba(L=L_train)\n",
    "plot_probabilities_histogram(probs_train[:, HIGH])\n",
    "\n",
    "preds = label_model.predict(L=L_train)\n",
    "A, B = np.unique(preds, return_counts=True)\n",
    "\n",
    "mask_values = ((preds == HIGH))\n",
    "keys = df[\"key\"][mask_values]\n",
    "\n",
    "file_path = \"<OUTPUT_PATH>\"\n",
    "build_and_save_trie(keys, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd31a1d-221c-4306-86c9-ad7caae06e32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
