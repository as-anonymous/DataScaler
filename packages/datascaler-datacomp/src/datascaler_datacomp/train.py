import collections
import json
import os
import pathlib
import pickle
import random
import re

import braceexpand
import datascaler_open_clip.params
import datascaler_open_clip.train
import webdataset as wds

import datascaler_datacomp.params
import datascaler_datacomp.scale
import datascaler_datacomp.utils


def prepare_filename(filename):
    filename = str(filename)
    if filename.startswith("s3://"):
        return f"pipe:aws s3 cp {filename} -"
    return filename


def split_filename(pattern, filename):
    filename = str(filename)
    pattern_match = pattern.search(filename)
    pos = pattern_match.start()
    return filename[:pos], filename[pos:]


def get_input_shards(data_dir: str | pathlib.Path, weights: str | None):
    data_dir = pathlib.Path(data_dir)
    # Handle multiple directories
    if "::" in str(data_dir):
        split_data_dir = str(data_dir).split("::")
        data_dirs = [pathlib.Path(subdir) for subdir in split_data_dir]
        if weights is None:
            split_weights = [None for _ in split_data_dir]
        else:
            split_weights = weights.split("::")
            assert len(split_weights) == len(split_data_dir)

        input_strs_and_weights = [
            get_input_shards(subdir, weight) for (subdir, weight) in zip(data_dirs, split_weights)
        ]

        input_strs, input_weights = zip(*input_strs_and_weights)
        input_strs = "::".join(input_strs)
        if weights is not None:
            weights = "::".join(input_weights)
        return input_strs, weights

    # Handle raw shards
    if data_dir.suffix == ".tar":
        return prepare_filename(data_dir), weights

    # Handle folders
    files_or_subdirs = list(data_dir.iterdir())
    data_str_components = []
    prefix_map = collections.defaultdict(list)
    pattern = re.compile(r"\d+$")  # Sequence of digits at the end of the string
    count_tars = 0
    for file_or_subdir in files_or_subdirs:
        if file_or_subdir.suffix == ".tar":
            shard = file_or_subdir.with_suffix("")
            prefix, suffix = split_filename(pattern, shard)
            prefix_map[prefix].append(suffix)
            count_tars += 1
        elif file_or_subdir.is_dir():
            # If the folder is generated by the resharder, the metadata file contains how many shards there are.
            metadata_file = file_or_subdir / "meta.json"
            if metadata_file.exists():
                with open(metadata_file, "r") as f:
                    metadata = json.load(f)
                shard_count = metadata["output_shard_count"]
                shard_format = metadata["output_shard_format"]
                first_shard = shard_format.format(0).replace(".tar", "")
                last_shard = shard_format.format(shard_count - 1).replace(".tar", "")
                filename = f"{{{first_shard}..{last_shard}}}.tar"
                subfolder_str = prepare_filename(file_or_subdir / filename)
                data_str_components.append(subfolder_str)
            else:
                sub_data_strs, _ = get_input_shards(file_or_subdir, weights)
                data_str_components.extend(sub_data_strs.split("::"))

    for prefix in sorted(list(prefix_map.keys())):
        last_tar = max([int(suffix) for suffix in prefix_map[prefix]])
        number_of_zeros = len(prefix_map[prefix][0])
        filename = f"{{{0:0{number_of_zeros}d}..{last_tar:0{number_of_zeros}d}}}.tar"
        filename = prepare_filename(prefix + filename)
        data_str_components.append(filename)
    data_str = "::".join(data_str_components)
    if weights is not None:
        weights = "::".join([weights for _ in data_str_components])
    return data_str, weights


def save_training_artifacts(
    params: datascaler_datacomp.params.DatacompTrainParams,
    scale_config: datascaler_datacomp.scale.DatacompScaleConfig,
    checkpoint: pathlib.Path,
):
    artifacts = {
        "scale": params.scale.value,
        "checkpoint": checkpoint.absolute(),
        "scale_config": scale_config.model_dump(),
        "data_dir": params.data_dir,
    }
    artifacts_file = checkpoint.parent.parent / "info.pkl"
    with artifacts_file.open("wb") as fw:
        pickle.dump(artifacts, fw)


def expand_urls(urls, weights=None):
    if weights is None:
        expanded_urls = wds.shardlists.expand_urls(urls)
        return expanded_urls, None
    if isinstance(urls, str):
        urllist = urls.split("::")
        weights = weights.split("::")
        assert len(weights) == len(urllist), (
            f"Expected the number of data components ({len(urllist)}) and weights({len(weights)}) to match."
        )
        weights = [float(weight) for weight in weights]
        all_urls, all_weights = [], []
        for url, weight in zip(urllist, weights):
            expanded_url = list(braceexpand.braceexpand(url))
            expanded_weights = [weight for _ in expanded_url]
            all_urls.extend(expanded_url)
            all_weights.extend(expanded_weights)
        return all_urls, all_weights
    else:
        all_urls = list(urls)
        return all_urls, weights


def main(params: datascaler_datacomp.params.DatacompTrainParams):
    _, rank, world_size = datascaler_datacomp.utils.world_info_from_env()
    if rank == 0:
        print("Running training on scale", params.scale)
        print(f"World size is {world_size}.")
    exp_name = params.exp_name if params.exp_name else f"{params.scale}_scale"
    scale_config = datascaler_datacomp.scale.get_scale_config(params.scale)
    scale_config.model = params.model_arch

    train_data, weights = get_input_shards(params.data_dir, params.data_weights)
    if params.val_ratio > 0:
        end_id = int(train_data.split("..")[1][:8])
        data_path = train_data.split("{")[0].split("::::")[-1]

        num_shards = int((end_id + 1) * params.val_ratio)
        larger_num_shards = (num_shards // (params.workers * world_size) + 1) * (
            params.workers * world_size
        )
        smaller_num_shards = (num_shards // (params.workers * world_size)) * (
            params.workers * world_size
        )

        if abs(larger_num_shards - num_shards) > abs(num_shards - smaller_num_shards):
            num_shards = smaller_num_shards
        else:
            num_shards = larger_num_shards

        random.seed(params.seed)
        all_indices = list(range(end_id + 1))
        val_indices = sorted(random.sample(all_indices, num_shards))
        train_indices = sorted(set(all_indices) - set(val_indices))

        val_data = [f"{data_path}{i:08d}.tar" for i in val_indices]
        train_data = [f"{data_path}{i:08d}.tar" for i in train_indices]

        val_data_stats = [f"{data_path}{i:08d}_stats.json" for i in val_indices]
        total_count, total_successes = 0, 0
        for file in val_data_stats:
            if file != "" and os.path.isfile(file):
                file = pathlib.Path(file)
                with file.open(encoding="utf-8") as fr:
                    meta = json.load(fr)

                total_count += meta["count"]
                total_successes += meta["successes"]
        val_num_samples = (total_successes // params.val_batch_size) * params.val_batch_size
    else:
        val_data = None
        val_num_samples = None

    if params.subset_file == "None":
        params.subset_file = None

    open_clip_train_params = datascaler_open_clip.params.OpenClipTrainParams(
        save_frequency=params.save_frequency,
        ddp_static_graph=True,
        local_loss=True,
        gather_with_grad=True,
        grad_checkpointing=True,
        train_data=train_data,
        val_data=val_data,
        val_frequency=1,
        val_num_samples=val_num_samples,
        train_num_samples=params.train_num_samples // params.num_checkpoints,
        warmup=scale_config.warmup,
        dataset_type="webdataset",  # type: ignore  # handled by Pydantic
        precision=params.precision,  # type: ignore  # handled by Pydantic
        workers=params.workers,
        model=scale_config.model,
        batch_size=scale_config.batch_size // (world_size * params.accum_freq),
        val_batch_size=params.val_batch_size // (world_size * params.accum_freq),
        epochs=params.num_checkpoints,
        lr=params.learning_rate,
        logs=params.output_dir,
        name=exp_name,
        seed=params.seed,
        accum_freq=params.accum_freq,
        log_every_n_steps=params.log_every_n_steps,
        save_most_recent=True,
        resume=params.resume,
        dataset_resampled=True,
        imagenet_val=params.imagenet_val,
        beta2=scale_config.beta2,
        train_data_upsampling_factors=weights,
        grad_clip_norm=params.grad_clip_norm,
        report_to="wandb" if params.report_to_wandb else "",
        wandb_project_name=params.wandb_project_name,
        lr_scheduler=params.lr_scheduler,
        use_mup=params.use_mup,
        width_mult=params.width_mult,
        min_width_mult=params.min_width_mult,
        attn_mult=params.attn_mult,
        output_mult=params.output_mult,
        subset_file=params.subset_file,
    )

    success = datascaler_open_clip.train.main(open_clip_train_params)

    if rank == 0:
        if success == -1:
            print("Error running training. Exiting.")

        log_dir = pathlib.Path(params.output_dir)
        final_checkpoint = log_dir / exp_name / "checkpoints" / "epoch_latest.pt"
        assert final_checkpoint.exists(), f"Did not find the checkpoint at {final_checkpoint}"
        save_training_artifacts(params, scale_config, final_checkpoint)

        print("Done training.")
